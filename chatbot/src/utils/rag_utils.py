"""Helper methods for RedmineRAG - ë¦¬íŒ©í† ë§ ë²„ì „"""
import logging
import re
import time
from datetime import datetime, timedelta, timezone
from collections import defaultdict
from google.genai import types
from prompts import PROMPT_TEMPLATES
from config import patterns as P
from config import constants as C

logger = logging.getLogger(__name__)


class RAGHelperMixin:
    """RAG ì—”ì§„ì„ ìœ„í•œ í—¬í¼ ë©”ì„œë“œ ëª¨ìŒ"""

    # ìƒìˆ˜ ê°€ì ¸ì˜¤ê¸°
    SESSION_ID_PREFIX = C.SESSION_ID_PREFIX
    HOSPITAL_MAPPING = C.HOSPITAL_MAPPING
    HOSPITAL_PRIORITY = C.HOSPITAL_PRIORITY

    # ì»´íŒŒì¼ëœ íŒ¨í„´ ìºì‹œ (í´ë˜ìŠ¤ ë³€ìˆ˜)
    _compiled_patterns = {}

    @classmethod
    def _get_compiled_patterns(cls, pattern_name: str):
        """íŒ¨í„´ì„ ì»´íŒŒì¼í•˜ì—¬ ìºì‹±"""
        if pattern_name not in cls._compiled_patterns:
            pattern_list = getattr(P, pattern_name, [])
            cls._compiled_patterns[pattern_name] = [
                re.compile(p, re.IGNORECASE) for p in pattern_list
            ]
        return cls._compiled_patterns[pattern_name]

    def _embed(self, text: str, task_type: str):
        if self.embedding_type == "gemini":
            result = self.genai_client.models.embed_content(
                model=self.embedding_model_name,
                contents=text,
                config=types.EmbedContentConfig(task_type=task_type)
            )
            return result.embeddings[0].values
        return self.embedding_model.encode(text).tolist()

    def _build_prompt(self, context: str, history_text: str, question: str) -> str:
        template_key = {"redmine": "redmine", "crf": "crf"}.get(self.use_case, "document")
        return PROMPT_TEMPLATES[template_key].format(
            context=context,
            history_text=history_text,
            question=question
        )

    def _build_general_conversation_prompt(self, question: str) -> str:
        """ì¼ë°˜ ëŒ€í™”ìš© í”„ë¡¬í”„íŠ¸"""
        return PROMPT_TEMPLATES["general"].format(question=question)

    def _format_context(self, documents: list, metadatas: list, limit: int) -> str:
        if self.use_case == "redmine":
            return "\n\n".join(
                f"[ì´ìŠˆ #{m.get('issue_id')} - {m.get('subject')}]\n{doc}"
                for m, doc in zip(metadatas[:limit], documents[:limit])
            )
        if self.use_case == "crf":
            return "\n\n".join(
                "[CRF {record_id} | ë³‘ì› {hospital} | ì‹œíŠ¸ {sheet}]\n{doc}".format(
                    record_id=m.get("record_id", "N/A"),
                    hospital=m.get("hospital", "N/A"),
                    sheet=m.get("sheet", "N/A"),
                    doc=doc,
                )
                for m, doc in zip(metadatas[:limit], documents[:limit])
            )
        return "\n\n".join(
            f"[ë¬¸ì„œ: {m.get('filename', 'Unknown')} (ì²­í¬ {m.get('chunk_index', 0)+1}/{m.get('total_chunks', 1)})]\n{doc}"
            for m, doc in zip(metadatas[:limit], documents[:limit])
        )

    # ========================================
    # ì§ˆë¬¸ ë¶„ë¥˜ ë©”ì„œë“œ (íŒ¨í„´ ê¸°ë°˜)
    # ========================================

    def _is_general_conversation(self, question: str) -> bool:
        """ì¼ë°˜ ëŒ€í™”ì¸ì§€ íŒë³„"""
        patterns = self._get_compiled_patterns('GENERAL_CONVERSATION_PATTERNS')
        return any(p.search(question.strip().lower()) for p in patterns)

    def is_crf_data_query(self, question: str) -> bool:
        """CRF/ì„ìƒ ë°ì´í„° ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ íŒë³„"""
        # ë³‘ì›ëª… íŒ¨í„´ ë™ì  ìƒì„±
        hospital_patterns = [re.escape(name) for name in self.HOSPITAL_MAPPING.keys()]

        # ëª¨ë“  CRF ê´€ë ¨ íŒ¨í„´ í•©ì¹˜ê¸°
        all_patterns = (
            P.CRF_BASE_PATTERNS +
            hospital_patterns +
            P.CRF_HOSPITAL_CODE_PATTERNS +
            P.CRF_MEDICAL_PATTERNS +
            P.CRF_FIELD_PATTERNS
        )
        return any(re.search(p, question, re.IGNORECASE) for p in all_patterns)

    def is_redmine_data_query(self, question: str) -> bool:
        """Redmine ì´ìŠˆ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ íŒë³„"""
        patterns = self._get_compiled_patterns('REDMINE_QUERY_PATTERNS')
        return any(p.search(question) for p in patterns)

    def _is_conversation_history_query(self, question: str) -> bool:
        """ëŒ€í™” ì´ë ¥ ì¡°íšŒ ì§ˆë¬¸ì¸ì§€ íŒë³„"""
        patterns = self._get_compiled_patterns('CONVERSATION_HISTORY_PATTERNS')
        return any(p.search(question) for p in patterns)

    def _is_version_or_comparison_query(self, question: str) -> bool:
        """ë²„ì „/ë¹„êµ ì§ˆë¬¸ì¸ì§€ íŒë³„"""
        patterns = self._get_compiled_patterns('VERSION_COMPARISON_PATTERNS')
        return any(p.search(question) for p in patterns)

    def _is_specific_technical_query(self, question: str) -> bool:
        """ê¸°ìˆ ì  ì§ˆë¬¸ì¸ì§€ íŒë³„"""
        patterns = self._get_compiled_patterns('TECHNICAL_QUERY_PATTERNS')
        return any(p.search(question) for p in patterns)

    def _is_metadata_query(self, question: str) -> bool:
        """ë©”íƒ€ë°ì´í„° ì§ˆë¬¸ ê°ì§€"""
        patterns = self._get_compiled_patterns('METADATA_QUERY_PATTERNS')
        return any(p.search(question) for p in patterns)

    def _is_sample_query(self, question: str) -> bool:
        """
        ì†Œìˆ˜ ì‚¬ë¡€(ìƒ˜í”Œ)ë§Œ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ì¸ì§€ íŒë³„
        - "ì‚¬ë¡€/ì¼€ì´ìŠ¤", "ë³´ì—¬ì¤˜/ì•Œë ¤ì¤˜", "nê°œë§Œ/ê±´ë§Œ" ë“±ì˜ í‘œí˜„ì´ ìˆëŠ” ê²½ìš° í†µê³„ë¡œ ë¶„ë¥˜í•˜ì§€ ì•ŠìŒ
        """
        patterns = self._get_compiled_patterns('SAMPLE_QUERY_PATTERNS')
        return any(p.search(question) for p in patterns)

    def _is_statistics_query(self, question: str) -> bool:
        """í†µê³„/ìš”ì•½ ì§ˆë¬¸ ê°ì§€"""
        if self._is_metadata_query(question):
            return False
        # ì‚¬ë¡€ ìƒ˜í”Œ ìš”ì²­(ì†ŒëŸ‰ ë³´ì—¬ì¤˜)ì€ í†µê³„ê°€ ì•„ë‹Œ ê²€ìƒ‰ ê²½ë¡œë¡œ ë³´ë‚¸ë‹¤
        if self._is_sample_query(question):
            return False
        patterns = self._get_compiled_patterns('STATISTICS_QUERY_PATTERNS')
        return any(p.search(question) for p in patterns)

    def _is_recent_query(self, question: str) -> bool:
        """ìµœì‹  ì§ˆë¬¸ì¸ì§€ íŒë³„"""
        return bool(P.RECENT_QUERY_PATTERN.search(question))

    # ========================================
    # ì¶”ì¶œ ë©”ì„œë“œ (ID, í† í° ë“±)
    # ========================================

    def _extract_issue_ids(self, question: str) -> list:
        """ì´ìŠˆ ë²ˆí˜¸ ì¶”ì¶œ"""
        matches = P.ISSUE_ID_PATTERN.findall(question)
        return list({str(int(g)) for m in matches for g in m if g})

    def _extract_crf_record_ids(self, question: str) -> list:
        """CRF record ID ì¶”ì¶œ"""
        matches = P.CRF_RECORD_PATTERN.findall(question)
        return list({m.upper() for m in matches})

    def _extract_hospital_code_from_question(self, question: str) -> str:
        """ì§ˆë¬¸ì—ì„œ ë³‘ì› ì½”ë“œ ì¶”ì¶œ"""
        normalized_question = question.replace(" ", "")

        for hospital_name in self.HOSPITAL_PRIORITY:
            normalized_name = hospital_name.replace(" ", "")
            if normalized_name in normalized_question:
                code = self.HOSPITAL_MAPPING.get(hospital_name)
                if code:
                    logger.info(f"  ğŸ¥ ë³‘ì› ê°ì§€: {hospital_name} (ì½”ë“œ: {code})")
                    return code
        return None

    def _extract_version_tokens(self, text: str) -> list:
        """ë²„ì „ ë¬¸ìì—´ ì¶”ì¶œ"""
        tokens = set()
        for match in P.VERSION_TOKEN_PATTERN.findall(text):
            tokens.add(match.lower())
            if match.lower().startswith('v'):
                tokens.add(match.lower()[1:])
            else:
                tokens.add(f"v{match.lower()}")
        return list(tokens)

    def _convert_hospital_names_to_codes(self, text: str) -> str:
        """ë³‘ì›ëª…ì„ ì½”ë“œë¡œ ë³€í™˜"""
        converted_text = text
        for hospital_name in self.HOSPITAL_PRIORITY:
            if hospital_name in converted_text:
                code = self.HOSPITAL_MAPPING.get(hospital_name)
                if code:
                    converted_text = converted_text.replace(
                        hospital_name,
                        f"{hospital_name} {code}"
                    )
        return converted_text

    def _chunk_statistics_text(self, text: str, max_chars: int = 80000) -> list:
        """í†µê³„/ë©”íƒ€ë°ì´í„° í…ìŠ¤íŠ¸ë¥¼ ê³ ì • ê¸¸ì´ë¡œ ê°•ì œ ì²­í¬ ë¶„í• """
        if len(text) <= max_chars:
            return [text]
        chunks = [
            text[i:i + max_chars]
            for i in range(0, len(text), max_chars)
        ]
        if C.DEBUG_CONFIG.get("crf_chunk_logging"):
            logger.info(f"  ğŸ“¦ ì²­í¬ ë¶„í• : ì´ {len(chunks)}ê°œ (ì…ë ¥ ê¸¸ì´: {len(text):,}, ì²­í¬ ìµœëŒ€: {max_chars})")
            if chunks:
                logger.info(f"    - ì²« ì²­í¬ ê¸¸ì´: {len(chunks[0])}, ë§ˆì§€ë§‰ ì²­í¬ ê¸¸ì´: {len(chunks[-1])}")
        return chunks

    # ========================================
    # ê²€ìƒ‰ ë° í•„í„°ë§ ë©”ì„œë“œ
    # ========================================

    def _build_search_query(self, question: str, chat_history: list) -> str:
        """ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (ëŒ€í™” ë§¥ë½ í¬í•¨)"""
        if self.use_case == "crf":
            question = self._convert_hospital_names_to_codes(question)

        if not chat_history:
            return question

        recent_turns = chat_history[-C.SEARCH_QUERY_CONFIG["recent_turns_to_include"]:]
        parts = [question]

        for turn in recent_turns:
            if turn.get("question"):
                hist_question = turn["question"]

                # use_caseë³„ í•„í„°ë§
                if self.use_case == "crf":
                    if self.is_crf_data_query(hist_question):
                        hist_question = self._convert_hospital_names_to_codes(hist_question)
                        parts.append(hist_question)
                elif self.use_case == "redmine":
                    if not self.is_crf_data_query(hist_question):
                        parts.append(hist_question)
                else:
                    if not self.is_crf_data_query(hist_question):
                        parts.append(hist_question)

            if turn.get("answer"):
                answer = turn["answer"]
                if self.use_case == "crf":
                    if any(kw in answer for kw in C.SEARCH_QUERY_CONFIG["crf_answer_keywords"]):
                        parts.append(answer[:200])
                elif self.use_case == "redmine":
                    if any(kw in answer for kw in C.SEARCH_QUERY_CONFIG["redmine_answer_keywords"]) and not any(
                        kw in answer for kw in C.SEARCH_QUERY_CONFIG["redmine_answer_exclude_keywords"]
                    ):
                        parts.append(answer[:200])

        search_query = " ".join(parts)
        logger.info(f"  ğŸ”„ ë§¥ë½ ê¸°ë°˜ ê²€ìƒ‰ (use_case={self.use_case}): {search_query[:300]}")
        return search_query

    def _get_model_keyword_cache(self):
        """ëª¨ë¸ í‚¤ì›Œë“œ ìºì‹œ ìƒì„± ë° ê´€ë¦¬"""
        cache = getattr(self, "_model_keyword_cache", None)
        cache_count = getattr(self, "_model_keyword_cache_count", None)

        try:
            current_count = self.collection.count()
        except Exception:
            current_count = None

        if cache is not None and (current_count is None or cache_count == current_count):
            return cache

        keywords = set()
        try:
            results = self.collection.get(include=["metadatas"])
            for meta in results.get("metadatas", []):
                subject = str(meta.get("subject", ""))
                for token in P.MODEL_KEYWORD_PATTERN.findall(subject):
                    if P.VERSION_FILTER_PATTERN.fullmatch(token):
                        continue
                    keywords.add(token.lower())
                    for part in re.split(r"[-_]", token):
                        if len(part) > 2 and not P.VERSION_FILTER_PATTERN.fullmatch(part):
                            keywords.add(part.lower())
            self._model_keyword_cache = keywords
            self._model_keyword_cache_count = current_count
        except Exception as e:
            logger.warning(f"  âš ï¸ ëª¨ë¸ í‚¤ì›Œë“œ ìºì‹œ ìƒì„± ì‹¤íŒ¨: {e}")
            self._model_keyword_cache = set()

        return self._model_keyword_cache

    def _extract_model_keywords(self, question: str) -> list:
        """ëª¨ë¸ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        cache = self._get_model_keyword_cache()
        if not cache:
            return []

        found = set()
        for token in P.MODEL_KEYWORD_PATTERN.findall(question):
            if P.VERSION_FILTER_PATTERN.fullmatch(token):
                continue
            key = token.lower()
            if key in cache:
                found.add(key)
            for part in re.split(r"[-_]", token):
                part_key = part.lower()
                if len(part) > 2 and part_key in cache:
                    found.add(part_key)

        return list(found)

    def _contains_keywords(self, documents: list, metadatas: list, keywords: list) -> bool:
        """ë¬¸ì„œì— í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸"""
        if not keywords:
            return False

        keywords_lower = [k.lower() for k in keywords]
        for doc, meta in zip(documents, metadatas):
            subject = str(meta.get("subject", "")).lower() if meta else ""
            doc_lower = str(doc).lower()
            for keyword in keywords_lower:
                if keyword in subject or keyword in doc_lower:
                    return True
        return False

    def _filter_by_keywords(self, documents: list, metadatas: list, distances: list, keywords: list):
        """í‚¤ì›Œë“œë¡œ ë¬¸ì„œ í•„í„°ë§"""
        if not keywords:
            return documents, metadatas, distances

        matched_docs, matched_metas, matched_dists = [], [], []
        keywords_lower = [k.lower() for k in keywords]

        for doc, meta, dist in zip(documents, metadatas, distances):
            subject = str(meta.get("subject", "")).lower() if meta else ""
            doc_lower = str(doc).lower()
            if any(keyword in subject or keyword in doc_lower for keyword in keywords_lower):
                matched_docs.append(doc)
                matched_metas.append(meta)
                matched_dists.append(dist)

        if matched_docs:
            logger.info(f"  âœ… í‚¤ì›Œë“œ ì¼ì¹˜ ë¬¸ì„œ í•„í„°ë§: {len(matched_docs)}ê°œ")
            return matched_docs, matched_metas, matched_dists

        return documents, metadatas, distances

    def _augment_with_keyword_matches(self, documents: list, metadatas: list, distances: list, keywords: list,
                                      limit_per_keyword: int = None):
        """í‚¤ì›Œë“œ ë³´ê°• ê²€ìƒ‰"""
        if limit_per_keyword is None:
            limit_per_keyword = C.KEYWORD_SEARCH_CONFIG['limit_per_keyword']

        if not keywords or self._contains_keywords(documents, metadatas, keywords):
            return documents, metadatas, distances

        existing_issue_ids = set()
        for meta in metadatas:
            if meta and meta.get("issue_id") is not None:
                existing_issue_ids.add(str(meta.get("issue_id")))

        extra_docs, extra_metas, extra_dists = [], [], []

        for keyword in keywords:
            try:
                result = self.collection.get(
                    where_document={"$contains": keyword},
                    include=["documents", "metadatas"],
                    limit=limit_per_keyword
                )
            except Exception as e:
                logger.warning(f"  âš ï¸ í‚¤ì›Œë“œ ë³´ê°• ê²€ìƒ‰ ì‹¤íŒ¨: {keyword} ({e})")
                continue

            docs = result.get("documents", [])
            metas = result.get("metadatas", [])
            for doc, meta in zip(docs, metas):
                issue_id = str(meta.get("issue_id")) if meta and meta.get("issue_id") is not None else None
                if issue_id and issue_id in existing_issue_ids:
                    continue
                if issue_id:
                    existing_issue_ids.add(issue_id)
                extra_docs.append(doc)
                extra_metas.append(meta)
                extra_dists.append(1.0)

        if extra_docs:
            logger.info(f"  ğŸ” í‚¤ì›Œë“œ ë³´ê°• ê²€ìƒ‰ ì¶”ê°€: {len(extra_docs)}ê°œ")
            documents = list(documents) + extra_docs
            metadatas = list(metadatas) + extra_metas
            distances = list(distances) + extra_dists

        return documents, metadatas, distances

    # ========================================
    # ì •ë ¬ ë©”ì„œë“œ
    # ========================================

    def _parse_timestamp(self, value: str):
        """íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹±"""
        try:
            return datetime.fromisoformat(value.replace('Z', '+00:00'))
        except Exception:
            return None

    def _sort_by_recency(self, documents: list, metadatas: list, distances: list):
        """ìµœì‹ ìˆœ ì •ë ¬"""
        if not metadatas:
            return documents, metadatas, distances

        scored = []
        for doc, meta, dist in zip(documents, metadatas, distances):
            timestamp = None
            if meta:
                timestamp = meta.get('updated_on') or meta.get('created_on')
            parsed = self._parse_timestamp(timestamp) if timestamp else None
            scored.append((parsed, doc, meta, dist))

        scored.sort(key=lambda x: (x[0] is not None, x[0]), reverse=True)
        return (
            [s[1] for s in scored],
            [s[2] for s in scored],
            [s[3] for s in scored],
        )

    # ========================================
    # ëŒ€í™” ê´€ë¦¬ ë©”ì„œë“œ
    # ========================================

    def save_conversation(self, session_id: str, turn_index: int, question: str, answer: str):
        """ëŒ€í™”ë¥¼ Vector DBì— ì €ì¥"""
        if not self.conversation_collection:
            return

        try:
            conversation_text = f"Q: {question}\nA: {answer}"
            embedding = self._embed(conversation_text, "RETRIEVAL_DOCUMENT")

            ttl_expire = (datetime.now() + timedelta(days=C.CHAT_HISTORY_CONFIG['ttl_days'])).isoformat()
            timestamp_id = int(time.time() * 1000000)
            doc_id = f"{session_id}_{timestamp_id}"

            self.conversation_collection.upsert(
                ids=[doc_id],
                embeddings=[embedding],
                documents=[conversation_text],
                metadatas=[{
                    "session_id": session_id,
                    "turn_index": turn_index,
                    "timestamp": datetime.now().isoformat(),
                    "question": question,
                    "answer": answer,
                    "ttl_expire": ttl_expire
                }]
            )

            logger.info(f"  ğŸ’¾ ëŒ€í™” ì €ì¥: {doc_id}")

        except Exception as e:
            logger.error(f"âŒ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def search_conversation_history(self, session_id: str, current_question: str, top_k: int = None) -> list:
        """ì„¸ì…˜ì˜ ê³¼ê±° ëŒ€í™”ì—ì„œ í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš© ê²€ìƒ‰"""
        if top_k is None:
            top_k = C.CHAT_HISTORY_CONFIG['max_relevant_history']

        if not self.conversation_collection:
            return []

        try:
            if self.conversation_collection.count() == 0:
                logger.info("  ğŸ” ê³¼ê±° ëŒ€í™” ì—†ìŒ (ì»¬ë ‰ì…˜ ë¹„ì–´ ìˆìŒ)")
                return []

            query_embedding = self._embed(current_question, "RETRIEVAL_QUERY")

            results = self.conversation_collection.query(
                query_embeddings=[query_embedding],
                where={"session_id": session_id},
                n_results=top_k
            )

            if not results['metadatas'] or not results['metadatas'][0]:
                return []

            history = []
            for metadata, distance in zip(results['metadatas'][0], results['distances'][0]):
                history.append({
                    "question": metadata.get("question", ""),
                    "answer": metadata.get("answer", ""),
                    "turn_index": metadata.get("turn_index", 0),
                    "timestamp": metadata.get("timestamp", ""),
                    "relevance_score": 1 - distance
                })

            logger.info(f"  ğŸ” ê³¼ê±° ëŒ€í™” ê²€ìƒ‰: {len(history)}ê°œ ë°œê²¬ (ì„¸ì…˜: {session_id})")
            return history

        except Exception as e:
            logger.error(f"âŒ ëŒ€í™” ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []

    def get_conversation_history_summary(self, session_id: str = None) -> dict:
        """ëŒ€í™” ì´ë ¥ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        if not self.conversation_collection:
            return {
                "total_conversations": 0,
                "sessions": [],
                "message": "ëŒ€í™” ì´ë ¥ ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤."
            }

        try:
            if session_id:
                results = self.conversation_collection.get(
                    where={"session_id": session_id},
                    include=["metadatas"]
                )
            else:
                results = self.conversation_collection.get(include=["metadatas"])

            if not results['metadatas']:
                return {
                    "total_conversations": 0,
                    "sessions": [],
                    "message": "ì €ì¥ëœ ëŒ€í™” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤."
                }

            sessions_data = {}
            for metadata in results['metadatas']:
                sid = metadata.get('session_id', 'Unknown')
                if sid not in sessions_data:
                    sessions_data[sid] = {
                        'session_id': sid,
                        'conversation_count': 0,
                        'conversations': [],
                        'first_timestamp': None,
                        'last_timestamp': None
                    }

                sessions_data[sid]['conversation_count'] += 1
                sessions_data[sid]['conversations'].append({
                    'turn_index': metadata.get('turn_index', 0),
                    'timestamp': metadata.get('timestamp', ''),
                    'question': metadata.get('question', ''),
                    'answer': metadata.get('answer', '')[:100] + '...' if len(metadata.get('answer', '')) > 100 else metadata.get('answer', '')
                })

                timestamp = metadata.get('timestamp', '')
                if not sessions_data[sid]['first_timestamp'] or timestamp < sessions_data[sid]['first_timestamp']:
                    sessions_data[sid]['first_timestamp'] = timestamp
                if not sessions_data[sid]['last_timestamp'] or timestamp > sessions_data[sid]['last_timestamp']:
                    sessions_data[sid]['last_timestamp'] = timestamp

            for sid in sessions_data:
                sessions_data[sid]['conversations'].sort(key=lambda x: x['turn_index'])

            sessions_list = sorted(sessions_data.values(), key=lambda x: x['last_timestamp'], reverse=True)

            return {
                "total_conversations": len(results['metadatas']),
                "total_sessions": len(sessions_data),
                "sessions": sessions_list
            }

        except Exception as e:
            logger.error(f"ëŒ€í™” ì´ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {
                "total_conversations": 0,
                "sessions": [],
                "error": str(e)
            }

    def get_user_list(self) -> list:
        """ëŒ€í™” ë¡œê·¸ì—ì„œ ì‚¬ìš©ì ëª©ë¡ ì¶”ì¶œ"""
        if not self.conversation_collection:
            return []

        try:
            if self.conversation_collection.count() == 0:
                logger.info("  ğŸ“‹ ì‚¬ìš©ì ì—†ìŒ (ëŒ€í™” ë¡œê·¸ ë¹„ì–´ ìˆìŒ)")
                return []

            results = self.conversation_collection.get()

            if not results or not results.get('metadatas'):
                return []

            user_stats = defaultdict(lambda: {
                'user_name': '',
                'total_conversations': 0,
                'first_seen': None,
                'last_seen': None
            })

            for metadata in results['metadatas']:
                session_id = metadata.get('session_id', '')
                timestamp = metadata.get('timestamp', '')

                if session_id.startswith(self.SESSION_ID_PREFIX):
                    user_name = session_id.replace(self.SESSION_ID_PREFIX, '')

                    user_stats[user_name]['user_name'] = user_name
                    user_stats[user_name]['total_conversations'] += 1

                    try:
                        ts = datetime.fromisoformat(timestamp)
                        if not user_stats[user_name]['first_seen'] or ts < user_stats[user_name]['first_seen']:
                            user_stats[user_name]['first_seen'] = ts
                        if not user_stats[user_name]['last_seen'] or ts > user_stats[user_name]['last_seen']:
                            user_stats[user_name]['last_seen'] = ts
                    except:
                        pass

            kst = timezone(timedelta(hours=9))
            user_list = []
            for user_name, stats in user_stats.items():
                first_seen_kst = stats['first_seen'].astimezone(kst) if stats['first_seen'] else None
                last_seen_kst = stats['last_seen'].astimezone(kst) if stats['last_seen'] else None

                user_list.append({
                    'user_name': stats['user_name'],
                    'total_conversations': stats['total_conversations'],
                    'first_seen': first_seen_kst.isoformat() if first_seen_kst else None,
                    'last_seen': last_seen_kst.isoformat() if last_seen_kst else None,
                })

            user_list.sort(key=lambda x: x['last_seen'] or '', reverse=True)

            logger.info(f"  ğŸ“‹ ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ: {len(user_list)}ëª…")
            return user_list

        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []

    def delete_user(self, user_name: str) -> bool:
        """íŠ¹ì • ì‚¬ìš©ìì˜ ëª¨ë“  ëŒ€í™” ë¡œê·¸ ì‚­ì œ"""
        if not self.conversation_collection:
            return False

        try:
            session_id = f"{self.SESSION_ID_PREFIX}{user_name}"

            results = self.conversation_collection.get(
                where={"session_id": session_id}
            )

            if not results or not results.get('ids'):
                logger.info(f"  ğŸ“‹ ì‚­ì œí•  ë°ì´í„° ì—†ìŒ: {user_name}")
                return True

            ids_to_delete = results['ids']
            self.conversation_collection.delete(ids=ids_to_delete)

            logger.info(f"  âœ… ì‚¬ìš©ì ì‚­ì œ ì™„ë£Œ: {user_name} ({len(ids_to_delete)}ê°œ ëŒ€í™”)")
            return True

        except Exception as e:
            logger.error(f"âŒ ì‚¬ìš©ì ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
            return False

    def get_document_count(self) -> int:
        """ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜ ë°˜í™˜"""
        try:
            return self.collection.count()
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return 0
