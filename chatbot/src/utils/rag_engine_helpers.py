"""RAG Engine Helper Methods - query ë©”ì„œë“œ ë¶„ë¦¬"""
import logging
import re
import base64
import json
from google.genai import types
from prompts import PROMPT_TEMPLATES
from config import constants as C

logger = logging.getLogger(__name__)


class QueryHelperMixin:
    """query() ë©”ì„œë“œì—ì„œ ì‚¬ìš©í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ ëª¨ìŒ"""

    # ========================================
    # íŠ¹ì • ì§ˆë¬¸ íƒ€ì…ë³„ ì²˜ë¦¬ ë©”ì„œë“œ
    # ========================================

    def _handle_general_conversation(self, question: str) -> dict:
        """ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ (ê²€ìƒ‰ ì—†ì´ LLM ì§ì ‘ ì‘ë‹µ)"""
        logger.info("ğŸ’¬ ì¼ë°˜ ëŒ€í™” ê°ì§€ (ê²€ìƒ‰ ìƒëµ)")
        prompt = self._build_general_conversation_prompt(question)
        response = self.genai_client.models.generate_content(
            model=self.model_name_pro,
            contents=prompt
        )
        return {
            "answer": response.text,
            "sources": [],
            "question": question
        }

    def _handle_conversation_history_query(self, question: str, session_id: str) -> dict:
        """ëŒ€í™” ì´ë ¥ ì¡°íšŒ ì²˜ë¦¬"""
        logger.info("ğŸ’¬ ëŒ€í™” ì´ë ¥ ì¡°íšŒ ì§ˆë¬¸ ê°ì§€")
        if not session_id:
            return {
                "answer": "ì„¸ì…˜ IDê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì ‘ì† í›„ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.",
                "sources": [],
                "question": question
            }

        history_summary = self.get_conversation_history_summary(session_id=session_id)

        if history_summary.get('total_conversations', 0) == 0:
            return {
                "answer": "ì´ ì„¸ì…˜ì—ì„œ ì €ì¥ëœ ëŒ€í™” ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "question": question
            }

        # ì‘ë‹µ í¬ë§·íŒ…
        answer_lines = [
            f"**ì„¸ì…˜ ëŒ€í™” ì´ë ¥ ({session_id})**",
            f"- ëŒ€í™” ìˆ˜: {history_summary['total_conversations']}ê°œ"
        ]

        if history_summary['sessions']:
            session_info = history_summary['sessions'][0]
            answer_lines.append(f"- ê¸°ê°„: {session_info['first_timestamp'][:19]} ~ {session_info['last_timestamp'][:19]}")
            answer_lines.append("")
            answer_lines.append("**ì§ˆë¬¸ ëª©ë¡:**")
            for j, conv in enumerate(session_info['conversations'], 1):
                answer_lines.append(f"  {j}. {conv['question']}")
            answer_lines.append("")

        return {
            "answer": "\n".join(answer_lines),
            "sources": [],
            "question": question
        }

    def _handle_crf_metadata_query(self, question: str, hospital_code: str = None, chat_history: list = None) -> dict:
        """CRF ë©”íƒ€ë°ì´í„° ì§ˆë¬¸ ì²˜ë¦¬"""
        logger.info("ğŸ—‚ï¸ CRF ë©”íƒ€ë°ì´í„° ì§ˆë¬¸ ê°ì§€ â†’ ì „ì²´ í˜„í™© ìš”ì•½")

        where_filter = {"hospital": hospital_code} if hospital_code else None
        data = self.collection.get(where=where_filter, include=["metadatas"])
        metadatas = data.get("metadatas") or []

        if not metadatas:
            return {
                "answer": "CRF ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "question": question
            }

        dataset_meta = self.get_dataset_metadata(metadatas)
        formatted_meta = self.format_metadata_for_llm(dataset_meta)

        return {
            "answer": formatted_meta,
            "sources": [],
            "question": question,
            "document_count": dataset_meta.get("total_records", 0)
        }

    def _handle_crf_statistics_query(self, question: str, hospital_code: str = None) -> dict:
        """CRF í†µê³„ ì§ˆë¬¸ ì²˜ë¦¬ (ì°¨íŠ¸ ìƒì„± í¬í•¨)"""
        logger.info("ğŸ“Š í†µê³„ ì§ˆë¬¸ ê°ì§€ â†’ Python ì§ì ‘ ê³„ì‚° + ì°¨íŠ¸ ìƒì„±")

        where_filter = {"hospital": hospital_code} if hospital_code else None
        data = self.collection.get(where=where_filter, include=["metadatas", "documents"])
        logger.info(f"  ğŸ“¦ ë°ì´í„° ë¡œë“œ: {len(data['documents'])}ê°œ")

        # í†µê³„ ê³„ì‚°
        stats = self.calculate_crf_statistics(
            data['documents'],
            data['metadatas'],
            hospital_code
        )
        stats_text = self.format_statistics_for_llm(stats)
        hospital_name = stats['hospital_name']

        # í•µì‹¬ í•„ë“œë§Œ ì¶”ì¶œ (Rate Limit íšŒí”¼ + LLM ì •í™•ë„ í–¥ìƒ)
        essential_fields = [
            # ë°”ì´ì˜¤ë§ˆì»¤ (ê°€ì¥ ì¤‘ìš”)
            'Ki-67 LI (%)', 'ER_IHC', 'PR_IHC', 'HER2_IHC',
            'ER (-/+)', 'PR (-/+)', 'HER2 (-/+)',
            # í™˜ì ì •ë³´
            'ë‚˜ì´ (ì§„ë‹¨ì‹œ)', 'ë³‘ì›ëª…',
            # ì¢…ì–‘ ì •ë³´
            'ì•” size (mm)_ì¥ê²½', 'T category', 'N category', 'M category',
            'NG (1/2/3)', 'HG (1/2/3/4)',
            'ì§„ë‹¨ëª… (histologic type',  # ì¡°ì§í•™ì  íƒ€ì…
            # ì¹˜ë£Œ ë° ì˜ˆí›„
            'ìˆ˜ìˆ ëª… (partial/total)', 'ë¦¼í”„ì ˆ ì „ì´ì—¬ë¶€_ìˆ˜ìˆ ë‹¹ì‹œ',
            'íê²½ ì—¬ë¶€', 'Stage', 'ì¬ë°œ ì—¬ë¶€'
        ]

        raw_metadata_list = []

        # documentsì—ì„œ íŒŒì‹± (í˜„ì¬ ë°ì´í„°ëŠ” documentsì— í…ìŠ¤íŠ¸ë¡œ ì €ì¥ë¨)
        for i, doc in enumerate(data['documents']):
            record = {'ë³‘ì›': data['metadatas'][i].get('hospital', '')}

            # "í•„ë“œëª…: ê°’" í˜•ì‹ íŒŒì‹±
            for line in doc.split('\n'):
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()

                    # í•µì‹¬ í•„ë“œë§Œ ì¶”ì¶œ
                    if key in essential_fields:
                        record[key] = value

            raw_metadata_list.append(record)

        # JSON ì§ë ¬í™” (ensure_ascii=Falseë¡œ í•œê¸€ ìœ ì§€)
        raw_metadata_json = json.dumps(raw_metadata_list, ensure_ascii=False, indent=2)
        logger.info(f"  ğŸ“‹ ì›ë³¸ ë©”íƒ€ë°ì´í„° í¬ê¸°: {len(raw_metadata_json):,} ë¬¸ì ({len(raw_metadata_list):,}ê°œ ë ˆì½”ë“œ)")
        logger.info(f"  ğŸ”‘ ì¶”ì¶œëœ í•„ë“œ ìˆ˜: {len(essential_fields)}ê°œ (ì „ì²´ 136ê°œ ì¤‘)")

        # í†µê³„/ë©”íƒ€ë°ì´í„°ê°€ í† í° í•œë„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì²­í¬ ë‹¨ìœ„ë¡œ Code Execution í˜¸ì¶œ
        # ìš”ì•½ í†µê³„ë§Œ ì „ë‹¬ (raw_metadataëŠ” LLMì— ë³´ë‚´ì§€ ì•ŠìŒ)
        stat_chunks = self._chunk_statistics_text(stats_text, max_chars=80000)
        total_parts = len(stat_chunks)
        chunked_prompts = []
        for idx, stat_chunk in enumerate(stat_chunks, start=1):
            chunked_prompts.append({
                "statistics": f"[í†µê³„ íŒŒíŠ¸ {idx}/{total_parts}]\n{stat_chunk}",
                "raw_metadata": ""  # ë©”íƒ€ë°ì´í„° ë¯¸ì „ì†¡
            })

        chart_images = []
        text_responses = []

        # ì°¨íŠ¸ ì‹¤í–‰ì€ ë„ˆë¬´ ë§ì•„ì§€ì§€ ì•Šë„ë¡ ì•ë¶€ë¶„ ì¼ë¶€ ì²­í¬ë§Œ ì‚¬ìš©
        chart_chunk_limit = len(chunked_prompts)

        for idx, chunk_info in enumerate(chunked_prompts[:chart_chunk_limit], start=1):
            if C.DEBUG_CONFIG.get("crf_chunk_logging"):
                logger.info(
                    f"ğŸ“Š ì²­í¬ í˜¸ì¶œ {idx}/{chart_chunk_limit} | "
                    f"í†µê³„ ê¸¸ì´: {len(chunk_info['statistics'])} / ë©”íƒ€ë°ì´í„° ê¸¸ì´: {len(chunk_info['raw_metadata'])}"
                )
            logger.info(f"ğŸ“Š ì°¨íŠ¸ ìƒì„± ì¤‘ (Code Execution) íŒŒíŠ¸ {idx}/{chart_chunk_limit}...")
            prompt = PROMPT_TEMPLATES["crf_statistics"].format(
                statistics=chunk_info["statistics"],
                raw_metadata=chunk_info["raw_metadata"],
                question=question,
                hospital_name=hospital_name
            )

            code_execution_tool = types.Tool(code_execution={})
            config = types.GenerateContentConfig(tools=[code_execution_tool])
            response = self.genai_client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=config
            )

            if hasattr(response, 'candidates') and response.candidates:
                for candidate in response.candidates:
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                text_responses.append(part.text)
                            elif hasattr(part, 'inline_data') and part.inline_data:
                                image_data = base64.b64encode(part.inline_data.data).decode('utf-8')
                                chart_images.append({
                                    'mime_type': part.inline_data.mime_type,
                                    'data': image_data
                                })
                                logger.info(f"  ğŸ“ˆ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„±ë¨: {part.inline_data.mime_type}")
                            elif hasattr(part, 'executable_code') and part.executable_code:
                                logger.info(f"  ğŸ LLM ì‹¤í–‰ ì½”ë“œ:\n{part.executable_code.code[:500]}...")
                            elif hasattr(part, 'code_execution_result') and part.code_execution_result:
                                logger.info(f"  âœ… ì½”ë“œ ì‹¤í–‰ ê²°ê³¼: {part.code_execution_result.outcome}")
            elif hasattr(response, 'parts'):
                for part in response.parts:
                    if hasattr(part, 'text') and part.text:
                        text_responses.append(part.text)
                    elif hasattr(part, 'inline_data') and part.inline_data:
                        image_data = base64.b64encode(part.inline_data.data).decode('utf-8')
                        chart_images.append({
                            'mime_type': part.inline_data.mime_type,
                            'data': image_data
                        })
                        logger.info(f"  ğŸ“ˆ ì°¨íŠ¸ ì´ë¯¸ì§€ ìƒì„±ë¨: {part.inline_data.mime_type}")

        return {
            "answer": "\n\n".join(text_responses) if text_responses else "ì°¨íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "sources": [],
            "question": question,
            "document_count": len(data['documents']) if data else 0,
            "charts": chart_images
        }

    # ========================================
    # ì§ì ‘ ì¡°íšŒ ë©”ì„œë“œ
    # ========================================

    def _perform_direct_lookup(self, question: str):
        """ì´ìŠˆ ë²ˆí˜¸ ë˜ëŠ” CRF record_id ì§ì ‘ ì¡°íšŒ"""
        direct_results = None

        if self.use_case == "redmine":
            issue_ids = self._extract_issue_ids(question)
            if issue_ids:
                logger.info(f"ğŸ” ì´ìŠˆ ë²ˆí˜¸ ê°ì§€ â†’ {issue_ids}")
                issue_ids_str = [str(i) for i in issue_ids]
                issue_ids_int = [int(i) for i in issue_ids_str if i.isdigit()]

                # ë¬¸ìì—´ ë§¤ì¹­
                direct_results = self.collection.get(
                    where={"issue_id": {"$in": issue_ids_str}},
                    include=["metadatas", "documents", "embeddings"]
                )

                # ì—†ìœ¼ë©´ ì •ìˆ˜ ë§¤ì¹­
                if not direct_results.get("documents") and issue_ids_int:
                    logger.info("  â¡ï¸ ë¬¸ìì—´ ë§¤ì¹­ ì‹¤íŒ¨ â†’ ì •ìˆ˜í˜• ë§¤ì¹­ ì¬ì‹œë„")
                    direct_results = self.collection.get(
                        where={"issue_id": {"$in": issue_ids_int}},
                        include=["metadatas", "documents", "embeddings"]
                    )

                found = len(direct_results.get("documents", []))
                if found:
                    logger.info(f"  âœ… ì´ìŠˆ ë²ˆí˜¸ ë§¤ì¹­ ì„±ê³µ: {found}ê±´")
                else:
                    logger.info("  âš ï¸ í•´ë‹¹ ì´ìŠˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")

        elif self.use_case == "crf":
            record_ids = self._extract_crf_record_ids(question)
            if record_ids:
                logger.info(f"ğŸ” CRF record_id ê°ì§€ â†’ {record_ids}")
                direct_results = self.collection.get(
                    where={"record_id": {"$in": record_ids}},
                    include=["metadatas", "documents", "embeddings"]
                )

                found = len(direct_results.get("documents", []))
                if found:
                    logger.info(f"  âœ… CRF record_id ë§¤ì¹­ ì„±ê³µ: {found}ê±´")
                else:
                    logger.info("  âš ï¸ í•´ë‹¹ record_idë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")

        return direct_results

    # ========================================
    # Top-K ê²°ì • ë©”ì„œë“œ
    # ========================================

    def _determine_top_k(self, question: str, top_k: int, recent_query: bool) -> int:
        """ì ì‘í˜• top_k ê²°ì •"""
        if top_k is not None:
            if recent_query and top_k < C.DEFAULT_TOP_K['recent']:
                top_k = C.DEFAULT_TOP_K['recent']
                logger.info("ğŸ“… ìµœì‹  ì§ˆë¬¸ ê°ì§€ â†’ top_k=100")
            return top_k

        # ìë™ ì„¤ì •
        if self.use_case == "crf":
            top_k = C.DEFAULT_TOP_K['crf']
            logger.info(f"ğŸ§¬ CRF ì§ˆë¬¸ â†’ top_k={top_k}")
        elif self._is_version_or_comparison_query(question):
            top_k = C.DEFAULT_TOP_K['version']
            logger.info(f"ğŸ“Š ë²„ì „/ë¹„êµ ì§ˆë¬¸ ê°ì§€ â†’ top_k={top_k}")
        elif self._is_specific_technical_query(question):
            top_k = C.DEFAULT_TOP_K['technical']
            logger.info(f"ğŸ”§ ê¸°ìˆ  ê²€ìƒ‰ ì§ˆë¬¸ ê°ì§€ â†’ top_k={top_k}")
        else:
            top_k = C.DEFAULT_TOP_K['general']
            logger.info(f"ğŸ“ ì¼ë°˜ ì§ˆë¬¸ â†’ top_k={top_k}")

        if recent_query and top_k < C.DEFAULT_TOP_K['recent']:
            top_k = C.DEFAULT_TOP_K['recent']
            logger.info("ğŸ“… ìµœì‹  ì§ˆë¬¸ ê°ì§€ â†’ top_k=100")

        return top_k

    # ========================================
    # ì¶œì²˜ ìƒì„± ë©”ì„œë“œ
    # ========================================

    def _generate_sources(self, documents: list, metadatas: list, distances: list, answer: str) -> list:
        """use_caseì— ë§ëŠ” ì¶œì²˜ ì •ë³´ ìƒì„±"""
        if self.use_case == "redmine":
            return self._generate_redmine_sources(documents, metadatas, distances, answer)
        elif self.use_case == "crf":
            return self._generate_crf_sources(documents, metadatas, distances)
        else:
            return self._generate_document_sources(documents, metadatas, distances)

    def _generate_redmine_sources(self, documents: list, metadatas: list, distances: list, answer: str) -> list:
        """Redmine ì¶œì²˜ ìƒì„±"""
        # ë‹µë³€ì—ì„œ ì–¸ê¸‰ëœ ì´ìŠˆ ë²ˆí˜¸ ì¶”ì¶œ
        mentioned_issues = set()
        for match in re.finditer(r'#(\d+)', answer):
            issue_num = int(match.group(1))
            mentioned_issues.add(issue_num)

        # ëª¨ë“  ê²€ìƒ‰ëœ ì´ìŠˆë¥¼ sourcesë¡œ ìƒì„±
        all_sources = [
            {
                "issue_id": meta.get("issue_id", "N/A"),
                "subject": meta.get("subject", "N/A"),
                "distance": float(dist),
                "content_preview": doc[:200] + "..." if len(doc) > 200 else doc,
                "url": f"{self.redmine_url}/issues/{meta.get('issue_id', '')}" if meta.get("issue_id") else None
            }
            for meta, dist, doc in zip(metadatas, distances, documents)
        ]

        # ë‹µë³€ì— ì–¸ê¸‰ëœ ì´ìŠˆê°€ ìˆìœ¼ë©´ ê·¸ê²ƒë§Œ, ì—†ìœ¼ë©´ ìƒìœ„ Nê°œ
        if mentioned_issues:
            filtered_sources = [
                src for src in all_sources
                if src["issue_id"] != "N/A" and int(src["issue_id"]) in mentioned_issues
            ]
            filtered_sources.sort(key=lambda x: int(x["issue_id"]))
            logger.info(f"  ğŸ“Œ ë‹µë³€ì— ì–¸ê¸‰ëœ ì´ìŠˆ: {len(filtered_sources)}ê°œ (ì „ì²´ ê²€ìƒ‰: {len(documents)}ê°œ)")
        else:
            top_n = min(5, len(all_sources))
            filtered_sources = [src for src in all_sources[:top_n] if src["issue_id"] != "N/A"]
            logger.info(f"  ğŸ“Œ ì°¸ì¡° ì´ìŠˆ (ì–¸ê¸‰ ì—†ìŒ): {len(filtered_sources)}ê°œ (ì „ì²´ ê²€ìƒ‰: {len(documents)}ê°œ)")

        return filtered_sources

    def _generate_crf_sources(self, documents: list, metadatas: list, distances: list) -> list:
        """CRF ì¶œì²˜ ìƒì„±"""
        top_n = min(5, len(documents))
        filtered_sources = []

        for meta, dist, doc in zip(metadatas[:top_n], distances[:top_n], documents[:top_n]):
            # ë¬¸ì„œì—ì„œ ë³‘ë¦¬ë²ˆí˜¸ ì¶”ì¶œ
            path_no_match = re.search(r'ë³‘ë¦¬ë²ˆí˜¸:\s*([^\n]+)', doc)
            path_no = path_no_match.group(1).strip() if path_no_match else "N/A"

            filtered_sources.append({
                "record_id": meta.get("record_id", "N/A"),
                "hospital": meta.get("hospital", "N/A"),
                "path_no": path_no,
                "sheet": meta.get("sheet", "N/A"),
                "row_index": meta.get("row_index", 0),
                "distance": float(dist),
                "content_preview": doc[:200] + "..." if len(doc) > 200 else doc
            })

        logger.info(f"  ğŸ“Œ ì°¸ì¡° CRF ë°ì´í„°: {len(filtered_sources)}ê°œ (ì „ì²´ ê²€ìƒ‰: {len(documents)}ê°œ)")
        return filtered_sources

    def _generate_document_sources(self, documents: list, metadatas: list, distances: list) -> list:
        """ì¼ë°˜ ë¬¸ì„œ ì¶œì²˜ ìƒì„±"""
        top_n = min(5, len(documents))
        filtered_sources = [
            {
                "filename": meta.get("filename", "Unknown"),
                "file_type": meta.get("file_type", "N/A"),
                "doc_category": meta.get("doc_category", "N/A"),
                "chunk_index": meta.get("chunk_index", 0),
                "total_chunks": meta.get("total_chunks", 1),
                "distance": float(dist),
                "content_preview": doc[:200] + "..." if len(doc) > 200 else doc
            }
            for meta, dist, doc in zip(metadatas[:top_n], distances[:top_n], documents[:top_n])
        ]

        logger.info(f"  ğŸ“Œ ì°¸ì¡° ë¬¸ì„œ: {len(filtered_sources)}ê°œ (ì „ì²´ ê²€ìƒ‰: {len(documents)}ê°œ)")
        return filtered_sources

    # ========================================
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
    # ========================================

    def _format_history_text(self, chat_history: list, relevant_history: list) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…"""
        if not chat_history and not relevant_history:
            return ""

        history_text = "\n<ëŒ€í™”_íˆìŠ¤í† ë¦¬>\n"

        # ìµœê·¼ ëŒ€í™”
        if chat_history:
            history_text += "[ìµœê·¼ ëŒ€í™”]\n"
            for i, turn in enumerate(chat_history[-C.CHAT_HISTORY_CONFIG['max_recent_turns']:], 1):
                history_text += f"- ì‚¬ìš©ì: {turn['question']}\n"
                history_text += f"  ì–´ì‹œìŠ¤í„´íŠ¸: {turn['answer']}\n"

        # ê´€ë ¨ ê³¼ê±° ëŒ€í™”
        if relevant_history:
            history_text += "\n[ê´€ë ¨ ê³¼ê±° ëŒ€í™”]\n"
            for hist in relevant_history[:C.CHAT_HISTORY_CONFIG['max_relevant_history']]:
                history_text += f"- ì‚¬ìš©ì: {hist['question']}\n"
                history_text += f"  ì–´ì‹œìŠ¤í„´íŠ¸: {hist['answer']}\n"

        history_text += "</ëŒ€í™”_íˆìŠ¤í† ë¦¬>\n"
        return history_text

    # ========================================
    # ë¬¸ì„œ ê²€ìƒ‰ ë° í›„ì²˜ë¦¬ ë©”ì„œë“œ
    # ========================================

    def _search_documents(self, question: str, chat_history: list, direct_results, top_k: int):
        """ë¬¸ì„œ ê²€ìƒ‰ (ì§ì ‘ ì¡°íšŒ ë˜ëŠ” ë²¡í„° ê²€ìƒ‰)"""
        documents, metadatas, distances = [], [], []

        if direct_results and direct_results.get("documents"):
            # ì§ì ‘ ì¡°íšŒ ê²°ê³¼ ì‚¬ìš©
            documents = direct_results.get("documents", [])
            metadatas = direct_results.get("metadatas", [])
            distances = [0.0 for _ in documents]
            logger.info(f"  âœ… ì§ì ‘ ì¡°íšŒ ê²°ê³¼: {len(documents)}ê±´")
        else:
            # ë²¡í„° ê²€ìƒ‰
            search_query = self._build_search_query(question, chat_history)
            logger.info(f"ğŸ” ê²€ìƒ‰ ì¤‘... (top_k={top_k})")
            query_embedding = self._embed(search_query, "RETRIEVAL_QUERY")

            # CRF: ë³‘ì› í•„í„°ë§
            hospital_code = None
            if self.use_case == "crf":
                hospital_code = self._extract_hospital_code_from_question(question)

            # Vector DB ê²€ìƒ‰
            where_filter = {"hospital": hospital_code} if hospital_code else None
            results = self.collection.query(
                query_embeddings=[query_embedding],
                where=where_filter,
                n_results=top_k
            )

            documents = results.get('documents', [[]])[0]
            metadatas = results.get('metadatas', [[]])[0]
            distances = results.get('distances', [[]])[0]

        logger.info(f"  âœ… ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(documents)}ê°œ")
        return documents, metadatas, distances

    def _post_process_documents(self, documents: list, metadatas: list, distances: list,
                                question: str, recent_query: bool):
        """ë¬¸ì„œ í›„ì²˜ë¦¬ (ì¬ì •ë ¬, í•„í„°ë§ ë“±)"""
        # 1. í‚¤ì›Œë“œ ë³´ê°• ê²€ìƒ‰ (Redmine ì „ìš©)
        if self.use_case == "redmine":
            keywords = self._extract_model_keywords(question)
            if keywords:
                documents, metadatas, distances = self._augment_with_keyword_matches(
                    documents, metadatas, distances, keywords
                )
                documents, metadatas, distances = self._filter_by_keywords(
                    documents, metadatas, distances, keywords
                )

        # 2. ë²„ì „ í† í° ì¬ì •ë ¬
        version_tokens = self._extract_version_tokens(question)
        if version_tokens and distances:
            logger.info(f"  ğŸ¯ ë²„ì „ í† í° ê°ì§€ â†’ ì¬ì •ë ¬: {version_tokens}")
            scored = []
            for doc, meta, dist in zip(documents, metadatas, distances):
                base_score = 1 - float(dist)
                boost = 0
                subject = str(meta.get("subject", "")).lower()
                version_meta = str(meta.get("version", "")).lower()
                doc_lower = str(doc).lower()
                for token in version_tokens:
                    if token in subject:
                        boost += 1
                    if token in version_meta:
                        boost += 1
                    if token in doc_lower:
                        boost += 1
                scored.append((base_score + 0.3 * boost, doc, meta, dist))

            scored.sort(key=lambda x: x[0], reverse=True)
            documents = [s[1] for s in scored]
            metadatas = [s[2] for s in scored]
            distances = [s[3] for s in scored]

        # 3. ìµœì‹ ìˆœ ì¬ì •ë ¬
        if recent_query:
            logger.info("  ğŸ“… ìµœì‹  ë°ì´í„° ìš”ì²­ ê°ì§€ â†’ ë‚ ì§œìˆœ ì¬ì •ë ¬")
            documents, metadatas, distances = self._sort_by_recency(documents, metadatas, distances)

        return documents, metadatas, distances

    def _generate_answer(self, question: str, documents: list, metadatas: list, distances: list,
                        chat_history: list, relevant_history: list) -> dict:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° LLM ë‹µë³€ ìƒì„±"""
        # 1. ì»¨í…ìŠ¤íŠ¸ ì œí•œ ê²°ì •
        if self.use_case == "crf":
            context_limit = len(documents)  # CRFëŠ” ì „ì²´ ì‚¬ìš©
            logger.info(f"  ğŸ§¬ CRF â†’ ê²€ìƒ‰ëœ ë¬¸ì„œ ì „ì²´ ì‚¬ìš©: {context_limit}ê°œ")
        else:
            context_limit = min(C.CONTEXT_LIMITS.get(self.use_case, 15), len(documents))

        # 2. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        context = self._format_context(documents, metadatas, context_limit)
        logger.info(f"  ğŸ“„ ì»¨í…ìŠ¤íŠ¸ì— ì‚¬ìš©ëœ ë¬¸ì„œ: {context_limit}ê°œ")

        # 3. ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
        history_text = self._format_history_text(chat_history, relevant_history)

        # 4. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_prompt(context, history_text, question)

        # 5. LLM ë‹µë³€ ìƒì„±
        logger.info("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...")
        response = self.genai_client.models.generate_content(
            model=self.model_name_pro,
            contents=prompt
        )
        answer = response.text

        # 6. ì¶œì²˜ ìƒì„±
        sources = self._generate_sources(documents, metadatas, distances, answer)

        return {
            "answer": answer,
            "sources": sources,
            "question": question,
            "document_count": len(documents)
        }
